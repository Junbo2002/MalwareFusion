import math

import torch
import torch.nn as nn
import numpy as np
import os
import torch.nn.utils.rnn as rnn_utils


class Transformer(nn.Module):
    def __init__(self, cfg=None):
        super(Transformer, self).__init__()
        """
        params
        """
        input_dim, output_dim, num_heads, dim_feedforward = 300, 512, 2, 300
        dropout_rate = 0.5
        num_layers = 2

        word2vec_path = cfg.DATASET.BASE_PATH + "matrix.npy"
        if os.path.exists(word2vec_path):
            pretrained_matrix = torch.Tensor(np.load(word2vec_path, allow_pickle=True))
            self.embed_size = pretrained_matrix.size(1)
            self.embedding = nn.Embedding.from_pretrained(pretrained_matrix, freeze=False, padding_idx=0)
        else:
            vocab_size = 2001
            embedding_dim = 300
            self.embed_size = vocab_size
            self.embedding = nn.Embedding(vocab_size, embedding_dim)

        self.positional_encoding = PositionalEncoding(self.embed_size, dropout=dropout_rate)
        self.input_linear = nn.Linear(input_dim, output_dim)

        # Transformer Encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=input_dim,
            nhead=num_heads,
            dim_feedforward=dim_feedforward,
            dropout=dropout_rate,
            batch_first=True
        )

        # Transformer Encoder
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        self.output_linear = nn.Linear(input_dim, output_dim)  # output_dim or 64
        # self.out_features = output_dim

    def forward(self, x):
        mask = (x == 0).to(x.device)
        lengths = x.size(1) - mask.sum(dim=1)
        x = self.embedding(x) * math.sqrt(self.embed_size)  # [25, 300, 300]
        x = self.positional_encoding(x)  # [25, 300, 300]
        # x = self.input_linear(x)

        x = self.transformer_encoder(x)  # [25, 300, 300] src_key_padding_mask=mask
        mask = mask[:, :x.size(1)]
        x = ~mask.unsqueeze(-1) * x  # [25, 300, 300]
        x = self.output_linear(x.sum(dim=1) / lengths.unsqueeze(-1))
        # x = self.output_linear(x)  # for TransformerFusion
        return x


class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        # Compute the positional encodings once in log space.
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1)
        div_term = torch.exp(
            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)
        )
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer("pe", pe)

    def forward(self, x):
        x = x + self.pe[:, : x.size(1)].requires_grad_(False)
        return self.dropout(x)
