import torch
import torch.nn as nn
import numpy as np
import torch.nn.functional as F
import math


class LearnableDistance(nn.Module):
    def __init__(self, feature_dim):
        super(LearnableDistance, self).__init__()
        self.in_features = feature_dim
        self.mlp = nn.Sequential(
            nn.Linear(feature_dim, feature_dim),
            nn.BatchNorm1d(feature_dim),
            nn.ReLU(inplace=True),
            # nn.Dropout(0.3),
            nn.Linear(feature_dim, feature_dim // 2),
            nn.BatchNorm1d(feature_dim // 2),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(feature_dim // 2, 1),
        )
        
        self.reset_parameters()

    def reset_parameters(self):
        for m in self.mlp:
            if isinstance(m, nn.Linear):
                nn.init.kaiming_uniform_(m.weight)

    def forward(self, x1, x2):
        # x1 and x2 shapes are now (batch_size, n, feature_dim)
        batch_size, n1, feature_dim = x1.size()
        _, n2, _ = x2.size()

        # Adjust the unsqueeze operation for batch processing
        x1 = x1.unsqueeze(2)  # Shape: (batch_size, n1, 1, feature_dim)
        x2 = x2.unsqueeze(1)  # Shape: (batch_size, 1, n2, feature_dim)

        # Compute the absolute difference using broadcasting
        abs_diff = torch.abs(x1 - x2)  # Shape: (batch_size, n1, n2, feature_dim)

        # Reshape for the MLP
        abs_diff = abs_diff.view(batch_size * n1 * n2, feature_dim)  # Shape: (batch_size*n1*n2, feature_dim)

        # Pass through MLP
        distances = self.mlp(abs_diff)

        # Reshape the distances back to the batched format
        distances = distances.view(batch_size, n1, n2)

        # Squeeze the last dimension if your MLP outputs more than one value per pair
        distances = distances.squeeze(-1)

        return distances


class GCN(nn.Module):
    def __init__(self, feature_dim=64):
        super(GCN, self).__init__()
        self.dist = LearnableDistance(feature_dim).cuda()
        self.proj = nn.Linear(feature_dim * 2, feature_dim * 2)
        self.W_in = nn.Linear(feature_dim, feature_dim * 2)
        self.W_out = nn.Linear(feature_dim * 2, feature_dim)
        self.reset_parameters()
        self.dropout = 0.3

    def reset_parameters(self):
        # if isinstance(self.proj, nn.Linear):
        #     nn.init.kaiming_uniform_(self.proj.weight)
        #     if self.proj.bias is not None:
        #         nn.init.constant_(self.proj.bias, 0)
        pass

    def forward(self, img_encodings, api_encodings, mode='train'):
        adj = self.dist(img_encodings, api_encodings)
        adj = F.softmax(F.leaky_relu(adj), dim=2)

        if mode == 'train':
            mask = torch.rand(adj.shape, device=adj.device) < self.dropout
            adj = adj * mask.float()

        # Use batched matrix multiplication for updating encodings
        projected_img_encodings = self.W_in(img_encodings)
        projected_api_encodings = self.W_in(api_encodings)
        projected_img_encodings = self.proj(torch.matmul(adj.transpose(1, 2), projected_img_encodings))
        projected_api_encodings = self.proj(torch.matmul(adj, projected_api_encodings))
        projected_img_encodings = self.W_out(projected_img_encodings)
        projected_api_encodings = self.W_out(projected_api_encodings)

        img_encodings, api_encodings = (
            img_encodings + projected_api_encodings,
            api_encodings + projected_img_encodings
        )

        return img_encodings, api_encodings


class GNNFusion(nn.Module):
    def __init__(self, cfg):
        super(GNNFusion, self).__init__()
        self.cfg = cfg
        self.num_nodes = 8
        self.num_layers = 2
        self.gcns = nn.ModuleList([GCN(512//self.num_nodes) for _ in range(self.num_layers)])

    def forward(self, img_encoded, api_encoded, mode='train'):
        encodings = torch.cat((img_encoded, api_encoded), dim=1)  # [25, 1024]
        batch_size, feature_dim = encodings.shape
        encodings = encodings.view(batch_size, self.num_nodes * 2, -1)  # [25, 16, 64]

        img_encodings = encodings[:, :self.num_nodes, :]  # [25, 16, 64]
        api_encodings = encodings[:, self.num_nodes:, :]

        for gcn in self.gcns:
            img_encodings, api_encodings = gcn(img_encodings, api_encodings, mode=mode)

        encodings_new = torch.cat((img_encodings, api_encodings), dim=1)
        return encodings_new.view(batch_size, -1)


class MLPFusion(nn.Module):
    def __init__(self, cfg):
        super(MLPFusion, self).__init__()
        self.cfg = cfg
        self.feature_dim = 512
        self.encoder = nn.Sequential(
            nn.Linear(self.feature_dim * 2, self.feature_dim),
            nn.BatchNorm1d(self.feature_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(self.feature_dim, self.feature_dim),
            nn.BatchNorm1d(self.feature_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(self.feature_dim, self.feature_dim)
        )

    def forward(self, img_encoded, api_encoded):
        encodings = torch.cat((img_encoded, api_encoded), dim=1)

        encodings = self.encoder(encodings)
        return encodings


class TransformerFusion(nn.Module):
    def __init__(self, cfg, input_dim=64, output_dim=512, num_heads=8, hidden_dim=256, dropout=0.3, num_layers=2):
        super(TransformerFusion, self).__init__()
        self.layer_norm = nn.LayerNorm(input_dim)
        self.embedding = nn.Linear(input_dim, hidden_dim)
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(hidden_dim, num_heads, dim_feedforward=hidden_dim, dropout=dropout),
            num_layers
        )
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, img_encoded, api_encoded):
        x = torch.cat((img_encoded, api_encoded), dim=1)
        x = self.layer_norm(x)
        x = self.embedding(x)  # [25, 349, 256]
        x = x.permute(1, 0, 2)  # Reshape to (seq_len, batch_size, hidden_dim)
        output = self.transformer(x)  # [349, 25, 256]
        output = output.mean(dim=0)  # Apply mean pooling over the sequence dimension
        output = self.fc(output)  # [25, 512]
        # output = F.relu(output)

        return output
